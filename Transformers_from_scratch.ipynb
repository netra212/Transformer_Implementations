{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b07508c",
   "metadata": {},
   "source": [
    "#### `Workflow`\n",
    "\n",
    "0. Import Important Libraries. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bded98",
   "metadata": {},
   "source": [
    "### **`Import Important Libraries.`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d39fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the important liberires from pytorch\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c86be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb7308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HuggingFace linraries\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a04d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361c1a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#typing\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441fe2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library for progress bars in loops\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779f9173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Library of warnings\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b35c5d",
   "metadata": {},
   "source": [
    "## 1. `Input Embedding.`\n",
    "\n",
    "`d_model`: 512\n",
    "\n",
    "In Input Embedding, we multiply those weights, by `np.sqrt(d_model)`\n",
    "\n",
    "Example Sentence:\n",
    "\n",
    "`English sentence`: The animal didn't cross the street because it was too tired. \n",
    "\n",
    "**1. `Tokens`**\n",
    "- Converting sentence into a single words. \n",
    "\n",
    "['The', 'animal', 'didn't', 'cross', 'the', 'street', 'because', 'it', 'was', 'too', 'tired' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007b9ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model:int, vocab_size:int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea312c4",
   "metadata": {},
   "source": [
    "## 2. `Positional Embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e345e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    '''\n",
    "    max_sequence_length: Like a length of the token.\n",
    "    '''\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        position = torch.arange(0, seq_len, dtype=float).unsqueeze(1) # [seq_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f70635",
   "metadata": {},
   "source": [
    "## 3. `Multihead Attention (Self-Head Attention)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9107649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
