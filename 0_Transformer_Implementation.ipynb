{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXG23klgbd8h"
      },
      "source": [
        "**important libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64WbKFXRZsQx",
        "outputId": "699fa9ab-d175-4c78-b806-98d8a4ddddb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6pawcjlaFrP"
      },
      "outputs": [],
      "source": [
        "# All the important liberires from pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CAXO673eabw2"
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDIO7HvVaeVS"
      },
      "outputs": [],
      "source": [
        "# HuggingFace Libraries\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88L08r6Ea2S2"
      },
      "outputs": [],
      "source": [
        "# Pathlib\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYZQpnsXbQFK"
      },
      "outputs": [],
      "source": [
        "# typing\n",
        "from typing import Any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXo2GnOEbW2V"
      },
      "outputs": [],
      "source": [
        "# Library for progress bars in loops\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "w0xzuz64bsfR"
      },
      "outputs": [],
      "source": [
        "#importing library of warnings\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y6j_LYFbwDo"
      },
      "source": [
        "**Input Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQw5S4DfbudC"
      },
      "outputs": [],
      "source": [
        "# Creating Input Embeddings\n",
        "class InputEmbeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model # Dimension of vectors (512) or the dimension of the model according to the attention all you need paper. \n",
        "        self.vocab_size = vocab_size # Size of the vocabulary or the total number of the unique words present in the dataset. \n",
        "        self.embedding = nn.Embedding(vocab_size, d_model) # PyTorch layer that converts integer indices to dense embeddings. Also, each word will have the 512 dimension. \n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x) * math.sqrt(self.d_model) # Normalizing the variance of the embedding "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS8V53JGchwY"
      },
      "source": [
        "**Positional Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pv6_cL89cgb0"
      },
      "outputs": [],
      "source": [
        "# Creating the Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    '''\n",
        "    d_model: Dimension of the model which is like 512, according to the attention all you need paper. \n",
        "    seq_len: seq_len is always taken maximum of the sequence. \n",
        "    dropout: used to reduce the overfitting of the model.\n",
        "    '''\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model # Dimensionality of the model\n",
        "        self.seq_len = seq_len # Maximum sequence length\n",
        "        self.dropout = nn.Dropout(dropout) # Dropout layer to prevent overfitting\n",
        "\n",
        "        # Creating a positional encoding matrix of shape (seq_len, d_model) filled with zeros\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "\n",
        "        # Creating a tensor representing positions (0 to seq_len - 1)\n",
        "        position = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1) # Transforming 'position' into a 2D tensor['seq_len, 1']\n",
        "\n",
        "        # Creating the division term for the positional encoding formula\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Apply sine to even indices in pe\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # Apply cosine to odd indices in pe\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Adding an extra dimension at the beginning of pe matrix for batch handling.\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # Registering 'pe' as buffer. Buffer is a tensor not considered as a model parameter\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self,x):\n",
        "        # Addind positional encoding to the input tensor X\n",
        "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
        "        return self.dropout(x) # Dropout for regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSUvVpl4ey-H"
      },
      "source": [
        "**Layer Normalization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmXErCeMexee"
      },
      "outputs": [],
      "source": [
        "# Creating Layer Normalization\n",
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "    def __init__(self, eps: float = 10**-6) -> None: # We define epsilon as 0.000001 to avoid division by zero\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "        # We define alpha as a trainable parameter and initialize it with ones\n",
        "        self.alpha = nn.Parameter(torch.ones(1)) # One-dimensional tensor that will be used to scale the input data\n",
        "\n",
        "        # We define bias as a trainable parameter and initialize it with zeros\n",
        "        self.bias = nn.Parameter(torch.zeros(1)) # One-dimensional tenso that will be added to the input data\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim = -1, keepdim = True) # Computing the mean of the input data. Keeping the number of dimensions unchanged\n",
        "        std = x.std(dim = -1, keepdim = True) # Computing the standard deviation of the input data. Keeping the number of dimensions unchanged\n",
        "\n",
        "        # Returning the normalized input\n",
        "        return self.alpha * (x-mean) / (std + self.eps) + self.bias\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaD76O04f5rG"
      },
      "source": [
        "**Feed Forward Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2GscLt3-f49R"
      },
      "outputs": [],
      "source": [
        "# Creating Feed Forward Layers\n",
        "class FeedForwardBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        # First linear transformation\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff) # W1 & b1\n",
        "        self.dropout = nn.Dropout(dropout) # Dropout to prevent overfitting\n",
        "        # Second linear transformation\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model) # W2 & b2\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (Batch, seq_len, d_model) --> (batch, seq_len, d_ff) -->(batch, seq_len, d_model)\n",
        "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwtCexHZg2SV"
      },
      "source": [
        "**Mulithead Attention**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1KJHg7yg1JQ"
      },
      "outputs": [],
      "source": [
        "# Creating the Multi-Head Attention block\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "    '''\n",
        "    Parameters:\n",
        "        h: represent the number of heads that we want to implments in encoder and decoder architecture. \n",
        "        512/8 = 64: means that each model's head will have now dimension of 64. \n",
        "    '''\n",
        "    def __init__(self, d_model: int, h: int, dropout: float) -> None: # h = number of heads\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "\n",
        "        # We ensure that the dimensions of the model is divisible by the number of heads\n",
        "        assert d_model % h == 0, 'd_model is not divisible by h'\n",
        "\n",
        "        # d_k is the dimension of each attention head's key, query, and value vectors\n",
        "        self.d_k = d_model // h # d_k formula, like in the original \"Attention Is All You Need\" paper\n",
        "\n",
        "        # Defining the weight matrices\n",
        "        self.w_q = nn.Linear(d_model, d_model) # W_q\n",
        "        self.w_k = nn.Linear(d_model, d_model) # W_k\n",
        "        self.w_v = nn.Linear(d_model, d_model) # W_v\n",
        "        self.w_o = nn.Linear(d_model, d_model) # W_o\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout) # Dropout layer to avoid overfitting\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):# mask => When we want certain words to NOT interact with others, we \"hide\" them\n",
        "\n",
        "        d_k = query.shape[-1] # The last dimension of query, key, and value\n",
        "\n",
        "        # We calculate the Attention(Q,K,V) as in the formula in the image above\n",
        "        attention_scores = (query @ key.transpose(-2,-1)) / math.sqrt(d_k) # @ = Matrix multiplication sign in PyTorch\n",
        "\n",
        "        # Before applying the softmax, we apply the mask to hide some interactions between words\n",
        "        if mask is not None: # If a mask IS defined...\n",
        "            attention_scores.masked_fill_(mask == 0, -1e9) # Replace each value where mask is equal to 0 by -1e9\n",
        "        attention_scores = attention_scores.softmax(dim = -1) # Applying softmax\n",
        "\n",
        "        if dropout is not None: # If a dropout IS defined...\n",
        "            attention_scores = dropout(attention_scores) # We apply dropout to prevent overfitting\n",
        "\n",
        "        return (attention_scores @ value), attention_scores # Multiply the output matrix by the V matrix, as in the formula\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "\n",
        "        query = self.w_q(q) # Q' matrix\n",
        "        key = self.w_k(k) # K' matrix\n",
        "        value = self.w_v(v) # V' matrix\n",
        "\n",
        "        # Splitting results into smaller matrices for the different heads\n",
        "        # Splitting embeddings (third dimension) into h parts\n",
        "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2) # Transpose => bring the head to the second dimension\n",
        "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2) # Transpose => bring the head to the second dimension\n",
        "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2) # Transpose => bring the head to the second dimension\n",
        "\n",
        "        # Obtaining the output and the attention scores\n",
        "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        # Obtaining the H matrix\n",
        "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "        return self.w_o(x) # Multiply the H matrix by the weight matrix W_o, resulting in the MH-A matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvcC06tzmxTg"
      },
      "source": [
        "**Residual Connection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FdsH8GNHmwX1"
      },
      "outputs": [],
      "source": [
        "# Building Residual Connection\n",
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout) # We use a dropout layer to prevent overfitting\n",
        "        self.norm = LayerNormalization() # We use a normalization layer\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        # We normalize the input and add it to the original input 'x'. This creates the residual connection process.\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWoqr2x7nkrP"
      },
      "source": [
        "**Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eHr8JB3anhPk"
      },
      "outputs": [],
      "source": [
        "# Building Encoder Block\n",
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    # This block takes in the MultiHeadAttentionBlock and FeedForwardBlock, as well as the dropout rate for the residual connections\n",
        "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        # Storing the self-attention block and feed-forward block\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)]) # 2 Residual Connections with dropout\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        # Applying the first residual connection with the self-attention block\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask)) # Three 'x's corresponding to query, key, and value inputs plus source mask\n",
        "\n",
        "        # Applying the second residual connection with the feed-forward block\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x # Output tensor after applying self-attention and feed-forward layers with residual connections.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw05zIS5pVGd"
      },
      "source": [
        "**Encoders**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nDLUGjd9pTr2"
      },
      "outputs": [],
      "source": [
        "# Building Encoder\n",
        "# An Encoder can have several Encoder Blocks\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    # The Encoder takes in instances of 'EncoderBlock'\n",
        "    def __init__(self, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers # Storing the EncoderBlocks\n",
        "        self.norm = LayerNormalization() # Layer for the normalization of the output of the encoder layers\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Iterating over each EncoderBlock stored in self.layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask) # Applying each EncoderBlock to the input tensor 'x'\n",
        "        return self.norm(x) # Normalizing output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDRX7aaIqUNl"
      },
      "source": [
        "**Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "m1sZoeGjqJaQ"
      },
      "outputs": [],
      "source": [
        "# Building Decoder Block\n",
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    # The DecoderBlock takes in two MultiHeadAttentionBlock. One is self-attention, while the other is cross-attention.\n",
        "    # It also takes in the feed-forward block and the dropout rate\n",
        "    def __init__(self,  self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)]) # List of three Residual Connections with dropout rate\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "\n",
        "        # Self-Attention block with query, key, and value plus the target language mask\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "\n",
        "        # The Cross-Attention block using two 'encoder_ouput's for key and value plus the source language mask. It also takes in 'x' for Decoder queries\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "\n",
        "        # Feed-forward block with residual connections\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Hdo3EBbasYP-"
      },
      "outputs": [],
      "source": [
        "# Building Decoder\n",
        "# A Decoder can have several Decoder Blocks\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    # The Decoder takes in instances of 'DecoderBlock'\n",
        "    def __init__(self, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # Storing the 'DecoderBlock's\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization() # Layer to normalize the output\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "\n",
        "        # Iterating over each DecoderBlock stored in self.layers\n",
        "        for layer in self.layers:\n",
        "            # Applies each DecoderBlock to the input 'x' plus the encoder output and source and target masks\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x) # Returns normalized output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hODxAz3tBi6"
      },
      "source": [
        "**Projection Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0ox43EYHtA0O"
      },
      "outputs": [],
      "source": [
        "# Buiding Linear Layer\n",
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int) -> None: # Model dimension and the size of the output vocabulary\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size) # Linear layer for projecting the feature space of 'd_model' to the output space of 'vocab_size'\n",
        "    def forward(self, x):\n",
        "        return torch.log_softmax(self.proj(x), dim = -1) # Applying the log Softmax function to the output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwMW-RA-tn6C"
      },
      "source": [
        "**Building the Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "v-96zTRCtmXg"
      },
      "outputs": [],
      "source": [
        "# Creating the Transformer Architecture\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    # This takes in the encoder and decoder, as well the embeddings for the source and target language.\n",
        "    # It also takes in the Positional Encoding for the source and target language, as well as the projection layer\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "    # Encoder\n",
        "    def encode(self, src, src_mask):\n",
        "        src = self.src_embed(src) # Applying source embeddings to the input source language\n",
        "        src = self.src_pos(src) # Applying source positional encoding to the source embeddings\n",
        "        return self.encoder(src, src_mask) # Returning the source embeddings plus a source mask to prevent attention to certain elements\n",
        "\n",
        "    # Decoder\n",
        "    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
        "        tgt = self.tgt_embed(tgt) # Applying target embeddings to the input target language (tgt)\n",
        "        tgt = self.tgt_pos(tgt) # Applying target positional encoding to the target embeddings\n",
        "\n",
        "        # Returning the target embeddings, the output of the encoder, and both source and target masks\n",
        "        # The target mask ensures that the model won't 'see' future elements of the sequence\n",
        "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "    # Applying Projection Layer with the Softmax function to the Decoder output\n",
        "    def project(self, x):\n",
        "        return self.projection_layer(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeWMFXmnv5mP"
      },
      "source": [
        "**Building And Initializig the Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xw__as_Jv2CK"
      },
      "outputs": [],
      "source": [
        "# Building & Initializing Transformer\n",
        "\n",
        "# Definin function and its parameter, including model dimension, number of encoder and decoder stacks, heads, etc.\n",
        "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
        "\n",
        "    # Creating Embedding layers\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size) # Source language (Source Vocabulary to 512-dimensional vectors)\n",
        "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size) # Target language (Target Vocabulary to 512-dimensional vectors)\n",
        "\n",
        "    # Creating Positional Encoding layers\n",
        "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout) # Positional encoding for the source language embeddings\n",
        "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout) # Positional encoding for the target language embeddings\n",
        "\n",
        "    # Creating EncoderBlocks\n",
        "    encoder_blocks = [] # Initial list of empty EncoderBlocks\n",
        "    for _ in range(N): # Iterating 'N' times to create 'N' EncoderBlocks (N = 6)\n",
        "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Self-Attention\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # FeedForward\n",
        "\n",
        "        # Combine layers into an EncoderBlock\n",
        "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block) # Appending EncoderBlock to the list of EncoderBlocks\n",
        "\n",
        "    # Creating DecoderBlocks\n",
        "    decoder_blocks = [] # Initial list of empty DecoderBlocks\n",
        "    for _ in range(N): # Iterating 'N' times to create 'N' DecoderBlocks (N = 6)\n",
        "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Self-Attention\n",
        "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Cross-Attention\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # FeedForward\n",
        "\n",
        "        # Combining layers into a DecoderBlock\n",
        "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block) # Appending DecoderBlock to the list of DecoderBlocks\n",
        "\n",
        "    # Creating the Encoder and Decoder by using the EncoderBlocks and DecoderBlocks lists\n",
        "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    # Creating projection layer\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size) # Map the output of Decoder to the Target Vocabulary Space\n",
        "\n",
        "    # Creating the transformer by combining everything above\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
        "\n",
        "    # Initialize the parameters\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer # Assembled and initialized Transformer. Ready to be trained and validated!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDk2XgZCBtE9"
      },
      "source": [
        "**Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "rbJHXywg_VYm"
      },
      "outputs": [],
      "source": [
        "# Defining Tokenizer\n",
        "def build_tokenizer(config, ds, lang):\n",
        "\n",
        "    # Crating a file path for the tokenizer\n",
        "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
        "\n",
        "    # Checking if Tokenizer already exists\n",
        "    if not Path.exists(tokenizer_path):\n",
        "\n",
        "        # If it doesn't exist, we create a new one\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token = '[UNK]')) # Initializing a new world-level tokenizer\n",
        "        tokenizer.pre_tokenizer = Whitespace() # We will split the text into tokens based on whitespace\n",
        "\n",
        "        # Creating a trainer for the new tokenizer\n",
        "        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[PAD]\",\n",
        "                                                     \"[SOS]\", \"[EOS]\"], min_frequency = 2) # Defining Word Level strategy and special tokens\n",
        "\n",
        "        # Training new tokenizer on sentences from the dataset and language specified\n",
        "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer = trainer)\n",
        "        tokenizer.save(str(tokenizer_path)) # Saving trained tokenizer to the file path specified at the beginning of the function\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path)) # If the tokenizer already exist, we load it\n",
        "    return tokenizer # Returns the loaded tokenizer or the trained tokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eoRv7POYJCQ"
      },
      "source": [
        "**casual_mask**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Li1W7F8hYLJQ"
      },
      "outputs": [],
      "source": [
        "def casual_mask(size):\n",
        "        # Creating a square matrix of dimensions 'size x size' filled with ones\n",
        "        mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n",
        "        return mask == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR97QgMDJvAY"
      },
      "source": [
        "**Load DataSets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CM_r0aQ7_vx8"
      },
      "outputs": [],
      "source": [
        "# Iterating through dataset to extract the original sentence and its translation\n",
        "def get_all_sentences(ds, lang):\n",
        "    for pair in ds:\n",
        "        yield pair['translation'][lang]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34XBKPvXYILQ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "_-G7wITV_vwE"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_ds(config):\n",
        "\n",
        "    # Loading the train portion of the OpusBooks dataset.\n",
        "    # The Language pairs will be defined in the 'config' dictionary we will build later\n",
        "    ds_raw = load_dataset('opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split = 'train')\n",
        "\n",
        "    # Building or loading tokenizer for both the source and target languages\n",
        "    tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n",
        "    tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
        "\n",
        "    # Splitting the dataset for training and validation\n",
        "    train_ds_size = int(0.9 * len(ds_raw)) # 90% for training\n",
        "    val_ds_size = len(ds_raw) - train_ds_size # 10% for validation\n",
        "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size]) # Randomly splitting the dataset\n",
        "\n",
        "    # Processing data with the BilingualDataset class, which we will define below\n",
        "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "\n",
        "    # Iterating over the entire dataset and printing the maximum length found in the sentences of both the source and target languages\n",
        "    max_len_src = 0\n",
        "    max_len_tgt = 0\n",
        "    for pair in ds_raw:\n",
        "        src_ids = tokenizer_src.encode(pair['translation'][config['lang_src']]).ids\n",
        "        tgt_ids = tokenizer_src.encode(pair['translation'][config['lang_tgt']]).ids\n",
        "        max_len_src = max(max_len_src, len(src_ids))\n",
        "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "    print(f'Max length of source sentence: {max_len_src}')\n",
        "    print(f'Max length of target sentence: {max_len_tgt}')\n",
        "\n",
        "    # Creating dataloaders for the training and validadion sets\n",
        "    # Dataloaders are used to iterate over the dataset in batches during training and validation\n",
        "    train_dataloader = DataLoader(train_ds, batch_size = config['batch_size'], shuffle = True) # Batch size will be defined in the config dictionary\n",
        "    val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle = True)\n",
        "\n",
        "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt # Returning the DataLoader objects and tokenizers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cChFz7MX_nb"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-NgoEyyD_vt6"
      },
      "outputs": [],
      "source": [
        "class BilingualDataset(Dataset):\n",
        "\n",
        "    # This takes in the dataset contaning sentence pairs, the tokenizers for target and source languages, and the strings of source and target languages\n",
        "    # 'seq_len' defines the sequence length for both languages\n",
        "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        self.ds = ds\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "\n",
        "        # Defining special tokens by using the target language tokenizer\n",
        "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
        "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
        "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
        "\n",
        "\n",
        "    # Total number of instances in the dataset (some pairs are larger than others)\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    # Using the index to retrive source and target texts\n",
        "    def __getitem__(self, index: Any) -> Any:\n",
        "        src_target_pair = self.ds[index]\n",
        "        src_text = src_target_pair['translation'][self.src_lang]\n",
        "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
        "\n",
        "        # Tokenizing source and target texts\n",
        "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "\n",
        "        # Computing how many padding tokens need to be added to the tokenized texts\n",
        "        # Source tokens\n",
        "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2 # Subtracting the two '[EOS]' and '[SOS]' special tokens\n",
        "        # Target tokens\n",
        "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 # Subtracting the '[SOS]' special token\n",
        "\n",
        "        # If the texts exceed the 'seq_len' allowed, it will raise an error. This means that one of the sentences in the pair is too long to be processed\n",
        "        # given the current sequence length limit (this will be defined in the config dictionary below)\n",
        "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
        "            raise ValueError('Sentence is too long')\n",
        "\n",
        "        # Building the encoder input tensor by combining several elements\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "            self.sos_token, # inserting the '[SOS]' token\n",
        "            torch.tensor(enc_input_tokens, dtype = torch.int64), # Inserting the tokenized source text\n",
        "            self.eos_token, # Inserting the '[EOS]' token\n",
        "            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Building the decoder input tensor by combining several elements\n",
        "        decoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token, # inserting the '[SOS]' token\n",
        "                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n",
        "            ]\n",
        "\n",
        "        )\n",
        "\n",
        "        # Creating a label tensor, the expected output for training the model\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n",
        "                self.eos_token, # Inserting the '[EOS]' token\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Adding padding tokens\n",
        "\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Ensuring that the length of each tensor above is equal to the defined 'seq_len'\n",
        "        assert encoder_input.size(0) == self.seq_len\n",
        "        assert decoder_input.size(0) == self.seq_len\n",
        "        assert label.size(0) == self.seq_len\n",
        "\n",
        "        return {\n",
        "            'encoder_input': encoder_input,\n",
        "            'decoder_input': decoder_input,\n",
        "            'encoder_mask': (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
        "            'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(decoder_input.size(0)),\n",
        "            'label': label,\n",
        "            'src_text': src_text,\n",
        "            'tgt_text': tgt_text\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKU_Qc6NYpXQ"
      },
      "source": [
        "**Validation Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "qMsJx2g3_vrt"
      },
      "outputs": [],
      "source": [
        "# Define function to obtain the most probable next token\n",
        "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
        "    # Retrieving the indices from the start and end of sequences of the target tokens\n",
        "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
        "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
        "\n",
        "    # Computing the output of the encoder for the source sequence\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "    # Initializing the decoder input with the Start of Sentence token\n",
        "    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n",
        "\n",
        "    # Looping until the 'max_len', maximum length, is reached\n",
        "    while True:\n",
        "        if decoder_input.size(1) == max_len:\n",
        "            break\n",
        "\n",
        "        # Building a mask for the decoder input\n",
        "        decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
        "\n",
        "        # Calculating the output of the decoder\n",
        "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
        "\n",
        "        # Applying the projection layer to get the probabilities for the next token\n",
        "        prob = model.project(out[:, -1])\n",
        "\n",
        "        # Selecting token with the highest probability\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        decoder_input = torch.cat([decoder_input, torch.empty(1,1). type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
        "\n",
        "        # If the next token is an End of Sentence token, we finish the loop\n",
        "        if next_word == eos_idx:\n",
        "            break\n",
        "\n",
        "    return decoder_input.squeeze(0) # Sequence of tokens generated by the decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "O8LR5lzA_vp1"
      },
      "outputs": [],
      "source": [
        "# Defining function to evaluate the model on the validation dataset\n",
        "# num_examples = 2, two examples per run\n",
        "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n",
        "    model.eval() # Setting model to evaluation mode\n",
        "    count = 0 # Initializing counter to keep track of how many examples have been processed\n",
        "\n",
        "    console_width = 80 # Fixed witdh for printed messages\n",
        "\n",
        "    # Creating evaluation loop\n",
        "    with torch.no_grad(): # Ensuring that no gradients are computed during this process\n",
        "        for batch in validation_ds:\n",
        "            count += 1\n",
        "            encoder_input = batch['encoder_input'].to(device)\n",
        "            encoder_mask = batch['encoder_mask'].to(device)\n",
        "\n",
        "            # Ensuring that the batch_size of the validation set is 1\n",
        "            assert encoder_input.size(0) ==  1, 'Batch size must be 1 for validation.'\n",
        "\n",
        "            # Applying the 'greedy_decode' function to get the model's output for the source text of the input batch\n",
        "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
        "\n",
        "            # Retrieving source and target texts from the batch\n",
        "            source_text = batch['src_text'][0]\n",
        "            target_text = batch['tgt_text'][0] # True translation\n",
        "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy()) # Decoded, human-readable model output\n",
        "\n",
        "            # Printing results\n",
        "            print_msg('-'*console_width)\n",
        "            print_msg(f'SOURCE: {source_text}')\n",
        "            print_msg(f'TARGET: {target_text}')\n",
        "            print_msg(f'PREDICTED: {model_out_text}')\n",
        "\n",
        "            # After two examples, we break the loop\n",
        "            if count == num_examples:\n",
        "                break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTSj0AIGZXFY"
      },
      "source": [
        "**Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fslG0H_c_vnj"
      },
      "outputs": [],
      "source": [
        "# We pass as parameters the config dictionary, the length of the vocabylary of the source language and the target language\n",
        "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
        "\n",
        "    # Loading model using the 'build_transformer' function.\n",
        "    # We will use the lengths of the source language and target language vocabularies, the 'seq_len', and the dimensionality of the embeddings\n",
        "    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "k1zst3-W_vgj"
      },
      "outputs": [],
      "source": [
        "# Define settings for building and training the transformer model\n",
        "def get_config():\n",
        "    return{\n",
        "        'batch_size': 8,\n",
        "        'num_epochs': 20,\n",
        "        'lr': 10**-4,\n",
        "        'seq_len': 350,\n",
        "        'd_model': 512, # Dimensions of the embeddings in the Transformer. 512 like in the \"Attention Is All You Need\" paper.\n",
        "        'lang_src': 'en',\n",
        "        'lang_tgt': 'it',\n",
        "        'model_folder': 'weights',\n",
        "        'model_basename': 'tmodel_',\n",
        "        'preload': None,\n",
        "        'tokenizer_file': 'tokenizer_{0}.json',\n",
        "        'experiment_name': 'runs/tmodel'\n",
        "    }\n",
        "\n",
        "\n",
        "# Function to construct the path for saving and retrieving model weights\n",
        "def get_weights_file_path(config, epoch: str):\n",
        "    model_folder = config['model_folder'] # Extracting model folder from the config\n",
        "    model_basename = config['model_basename'] # Extracting the base name for model files\n",
        "    model_filename = f\"{model_basename}{epoch}.pt\" # Building filename\n",
        "    return str(Path('.')/ model_folder/ model_filename) # Combining current directory, the model folder, and the model filename\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "7wZXtwb__veD"
      },
      "outputs": [],
      "source": [
        "def train_model(config):\n",
        "    # Setting up device to run on GPU to train faster\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device {device}\")\n",
        "\n",
        "    # Creating model directory to store weights\n",
        "    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Retrieving dataloaders and tokenizers for source and target languages using the 'get_ds' function\n",
        "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "\n",
        "    # Initializing model on the GPU using the 'get_model' function\n",
        "    model = get_model(config,tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "\n",
        "    # Tensorboard\n",
        "    writer = SummaryWriter(config['experiment_name'])\n",
        "\n",
        "    # Setting up the Adam optimizer with the specified learning rate from the '\n",
        "    # config' dictionary plus an epsilon value\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps = 1e-9)\n",
        "\n",
        "    # Initializing epoch and global step variables\n",
        "    initial_epoch = 0\n",
        "    global_step = 0\n",
        "\n",
        "    # Checking if there is a pre-trained model to load\n",
        "    # If true, loads it\n",
        "    if config['preload']:\n",
        "        model_filename = get_weights_file_path(config, config['preload'])\n",
        "        print(f'Preloading model {model_filename}')\n",
        "        state = torch.load(model_filename) # Loading model\n",
        "\n",
        "        # Sets epoch to the saved in the state plus one, to resume from where it stopped\n",
        "        initial_epoch = state['epoch'] + 1\n",
        "        # Loading the optimizer state from the saved model\n",
        "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        # Loading the global step state from the saved model\n",
        "        global_step = state['global_step']\n",
        "\n",
        "    # Initializing CrossEntropyLoss function for training\n",
        "    # We ignore padding tokens when computing loss, as they are not relevant for the learning process\n",
        "    # We also apply label_smoothing to prevent overfitting\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id('[PAD]'), label_smoothing = 0.1).to(device)\n",
        "\n",
        "    # Initializing training loop\n",
        "\n",
        "    # Iterating over each epoch from the 'initial_epoch' variable up to\n",
        "    # the number of epochs informed in the config\n",
        "    for epoch in range(initial_epoch, config['num_epochs']):\n",
        "\n",
        "        # Initializing an iterator over the training dataloader\n",
        "        # We also use tqdm to display a progress bar\n",
        "        batch_iterator = tqdm(train_dataloader, desc = f'Processing epoch {epoch:02d}')\n",
        "\n",
        "        # For each batch...\n",
        "        for batch in batch_iterator:\n",
        "            model.train() # Train the model\n",
        "\n",
        "            # Loading input data and masks onto the GPU\n",
        "            encoder_input = batch['encoder_input'].to(device)\n",
        "            decoder_input = batch['decoder_input'].to(device)\n",
        "            encoder_mask = batch['encoder_mask'].to(device)\n",
        "            decoder_mask = batch['decoder_mask'].to(device)\n",
        "\n",
        "            # Running tensors through the Transformer\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
        "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
        "            proj_output = model.project(decoder_output)\n",
        "\n",
        "            # Loading the target labels onto the GPU\n",
        "            label = batch['label'].to(device)\n",
        "\n",
        "            # Computing loss between model's output and true labels\n",
        "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "\n",
        "            # Updating progress bar\n",
        "            batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "            writer.add_scalar('train loss', loss.item(), global_step)\n",
        "            writer.flush()\n",
        "\n",
        "            # Performing backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            # Updating parameters based on the gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            # Clearing the gradients to prepare for the next batch\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            global_step += 1 # Updating global step count\n",
        "\n",
        "        # We run the 'run_validation' function at the end of each epoch\n",
        "        # to evaluate model performance\n",
        "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
        "\n",
        "        # Saving model\n",
        "        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n",
        "        # Writting current model state to the 'model_filename'\n",
        "        torch.save({\n",
        "            'epoch': epoch, # Current epoch\n",
        "            'model_state_dict': model.state_dict(),# Current model state\n",
        "            'optimizer_state_dict': optimizer.state_dict(), # Current optimizer state\n",
        "            'global_step': global_step # Current global step\n",
        "        }, model_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2e5a126b9df541478efda040066e5438",
            "e4eef6b7f0a24d02a23845b99ac4978c",
            "e5fd79a50c3d4fd19d3df19862238109",
            "0fabd6a30a0649469a446c52a5afb455",
            "b332b3772a664901b9870841b96440bb",
            "14002e17454245beb17ca33393a6d4e3",
            "135b5690d178472785fdbd679ac009c7",
            "80cbc6d514a543beb16f14b28a8dbf86",
            "958e21e68fdb4c5b9f5cdb3c4ae54e6e",
            "0d8b488bbc734277986d244969d87416",
            "cd3d42e3afd646618295805eff5f2027",
            "4173b16c96c1471d911b399a6d8b0b0f",
            "7bda257be65d475e88759738a2f02434",
            "e581731685c2434ab864c77b93900987",
            "5e0c241e1d994604852abc08ad6d8244",
            "7404303c4ef647d1a04e19a992545270",
            "c13dafdc47ca427bbe174e85b7c99cb3",
            "0fe546c5595f493abbd9b3085e03c1b9",
            "57b1e06305bd4f9c9f5d120a45b286f0",
            "a71c9a70715440f7be0ee36a69ebaa98",
            "7f1f2a8dfc9d4d7aa06f2cd506d58304",
            "d07eac00c6464627acf8c91820e0d33d",
            "591a611549644bf29b78dcf08209c393",
            "5d8bdfe23dab477889453d9e6753d89d",
            "cb9267758a074b53b6e4792ddae9ca85",
            "0332c323548c43cdbd0765515e2e06d3",
            "8068edb79aeb4924adefebfe737f662a",
            "f05d60d0049548688f0c2b653bbf7bf5",
            "cd951a4f5dfb4205bcdecedbb6c05000",
            "e7045851da074c86a4d96f130d414bdc",
            "6657b866176a4eaea7f5c0c1319c882d",
            "4a890b04ddbb41a7b8f0d03d7f1a8cff",
            "6e08926aac1141a684bd95dfae430b08"
          ]
        },
        "id": "6jBu6lo6Z-hm",
        "outputId": "bc4ea2a2-59a9-4202-c30f-a5dc2dde3f98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cuda\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e5a126b9df541478efda040066e5438",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/28.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4173b16c96c1471d911b399a6d8b0b0f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/5.73M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "591a611549644bf29b78dcf08209c393",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/32332 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length of source sentence: 309\n",
            "Max length of target sentence: 274\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing epoch 00: 100%|██████████| 3638/3638 [25:45<00:00,  2.35it/s, loss=6.388]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: A week passed, and no news arrived of Mr. Rochester: ten days, and still he did not come.\n",
            "TARGET: Trascorse una settimana senza che avessi notizie del signor Rochester. Dopo dieci giorni non era ancora tornato.\n",
            "PREDICTED: La porta , ma non si era più più a lui , ma non si era più .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Certain accessory points of the design served well to convey the idea that this excavation lay at an exceeding depth below the surface of the earth.\n",
            "TARGET: Certi dettagli accessori servivano a far capire che quella galleria si trovava ad una profondità eccessiva sotto la superficie della terra.\n",
            "PREDICTED: , , e , e , e , , , , .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing epoch 01: 100%|██████████| 3638/3638 [25:47<00:00,  2.35it/s, loss=4.601]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Mrs. Karenina did not wait for her brother to come in, but, on seeing him, descended from the carriage with a firm light step.\n",
            "TARGET: Ma la Karenina non aspettò che il fratello si avvicinasse e, non appena lo vide, col suo passo leggero e deciso scese subito dalla vettura.\n",
            "PREDICTED: Il signor Rochester non era nulla di lei , ma non si era un ’ altra , ma , il suo viso , si mise a un ’ altra .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Let the naturalists explain these things, and the reason and manner of them. All I can do is to describe the fact, which was even surprising to me when I found it, though I knew not from whence it proceeded; it was doubtless the effect of ardent wishes, and of strong ideas formed in my mind, realising the comfort which the conversation of one of my fellow-Christians would have been to me.\n",
            "TARGET: Lascio ai naturalisti lo spiegar queste cose e il come e il donde succedano: soltanto io posso descrivere un fatto che rese attonito me ancora quando m’accadde, benchè non sapessi da che procedea. Esso fu indubitatamente l’effetto di ardenti desideri e di gagliarde immaginazioni che, improntatesi nella mia mente, le mostravano qual conforto mi avrebbe arrecato il conversare con uno soltanto dei Cristiani miei simili.\n",
            "PREDICTED: \" , e non ci , e non ho mai mai mai mai mai mai , e non posso essere più più di , e non mi , e non mi , e non mi , e non mi , e non mi , e non poteva essere più più più di di , e di , e di , e di .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing epoch 02: 100%|██████████| 3638/3638 [25:48<00:00,  2.35it/s, loss=4.244]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next.\n",
            "TARGET: Una delle due: o il pozzo era straordinariamente profondo o ella ruzzolava giù con grande lentezza, perchè ebbe tempo, cadendo, di guardarsi intorno e di pensar meravigliata alle conseguenze.\n",
            "PREDICTED: , , o , non era più più forte , si , e , a , e , a , e a .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Look at Mazankov, at Krupov!\n",
            "TARGET: Guarda Mazankov, Krupov.\n",
            "PREDICTED: , !\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing epoch 03: 100%|██████████| 3638/3638 [25:48<00:00,  2.35it/s, loss=3.707]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: 'Wait a moment!\n",
            "TARGET: — Ah, lascia stare!\n",
            "PREDICTED: — Aspetta , è un attimo !\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Alice was very nearly getting up and saying, 'Thank you, sir, for your interesting story,' but she could not help thinking there must be more to come, so she sat still and said nothing.\n",
            "TARGET: Alice stava per levarsi e dirle: — Grazie della vostra storia interessante, — quando pensò che ci doveva essere qualche altra cosa, e sedette tranquillamente senza dir nulla.\n",
            "PREDICTED: Alice era un po ' di nuovo e disse : — Non vi , signore , ma non si può non più più più più più di più più di più , e non si disse nulla .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing epoch 04: 100%|██████████| 3638/3638 [25:47<00:00,  2.35it/s, loss=5.002]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: She wore a white dress trimmed with wide embroidery, and as she sat in a corner of the verandah behind some plants, did not hear Vronsky coming.\n",
            "TARGET: Vestiva un abito bianco con un largo ricamo; sedeva in un angolo della terrazza di là dai fiori e non aveva avvertito l’avvicinarsi di lui.\n",
            "PREDICTED: Ella si voltò con un vestito di seta , e si voltò a guardare il viale , senza guardare Vronskij , senza rispondere .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: I did so: she put her arm over me, and I nestled close to her.\n",
            "TARGET: Ubbidii; ella mi abbracciò e mi attirò a sé.\n",
            "PREDICTED: Mi alzai e mi guardò la mano e mi .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing epoch 05: 100%|██████████| 3638/3638 [25:47<00:00,  2.35it/s, loss=4.341]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: The gale still rising, seemed to my ear to muffle a mournful under-sound; whether in the house or abroad I could not at first tell, but it recurred, doubtful yet doleful at every lull; at last I made out it must be some dog howling at a distance.\n",
            "TARGET: \"Sulle prime non potei rendermi conto se que' suoni venivano dalla casa o dal di fuori; si rinnovavano continuamente, del pari dolorosi e vaghi; alla fine pensai che fosse un cane, ululante in lontananza.\n",
            "PREDICTED: Il silenzio era ancora più forte , che mi pareva di nuovo un rumore di una casa , o no , o no , non mi riuscì a pensare , ma dopo un tratto , e a un tratto , mi , perchè un cane di un cane .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: It made our mouths water to hear him talk about the things, and we handed him out the stove and the frying-pan and all the eggs that had not smashed and gone over everything in the hamper, and begged him to begin.\n",
            "TARGET: Ci venne l’acquolina in bocca sentendolo parlar così, e gli affidammo la cucinetta, la padella e tutte le uova che non s’erano rotte insudiciando tutto nel paniere, e lo pregammo di cominciare.\n",
            "PREDICTED: il cavo , lo di parlare , e lo , e lo , e lo di nuovo il cavo , e non ci fu più di lui , e che il cavo di , lo .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing epoch 06:  18%|█▊        | 643/3638 [04:33<21:11,  2.36it/s, loss=3.458]"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    warnings.filterwarnings('ignore') # Filtering warnings\n",
        "    config = get_config() # Retrieving config settings\n",
        "    train_model(config) # Training model with the config arguments"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0332c323548c43cdbd0765515e2e06d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a890b04ddbb41a7b8f0d03d7f1a8cff",
            "placeholder": "​",
            "style": "IPY_MODEL_6e08926aac1141a684bd95dfae430b08",
            "value": " 32332/32332 [00:00&lt;00:00, 295450.36 examples/s]"
          }
        },
        "0d8b488bbc734277986d244969d87416": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fabd6a30a0649469a446c52a5afb455": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d8b488bbc734277986d244969d87416",
            "placeholder": "​",
            "style": "IPY_MODEL_cd3d42e3afd646618295805eff5f2027",
            "value": " 28.1k/28.1k [00:00&lt;00:00, 1.02MB/s]"
          }
        },
        "0fe546c5595f493abbd9b3085e03c1b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "135b5690d178472785fdbd679ac009c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14002e17454245beb17ca33393a6d4e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e5a126b9df541478efda040066e5438": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4eef6b7f0a24d02a23845b99ac4978c",
              "IPY_MODEL_e5fd79a50c3d4fd19d3df19862238109",
              "IPY_MODEL_0fabd6a30a0649469a446c52a5afb455"
            ],
            "layout": "IPY_MODEL_b332b3772a664901b9870841b96440bb"
          }
        },
        "4173b16c96c1471d911b399a6d8b0b0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7bda257be65d475e88759738a2f02434",
              "IPY_MODEL_e581731685c2434ab864c77b93900987",
              "IPY_MODEL_5e0c241e1d994604852abc08ad6d8244"
            ],
            "layout": "IPY_MODEL_7404303c4ef647d1a04e19a992545270"
          }
        },
        "4a890b04ddbb41a7b8f0d03d7f1a8cff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57b1e06305bd4f9c9f5d120a45b286f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "591a611549644bf29b78dcf08209c393": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d8bdfe23dab477889453d9e6753d89d",
              "IPY_MODEL_cb9267758a074b53b6e4792ddae9ca85",
              "IPY_MODEL_0332c323548c43cdbd0765515e2e06d3"
            ],
            "layout": "IPY_MODEL_8068edb79aeb4924adefebfe737f662a"
          }
        },
        "5d8bdfe23dab477889453d9e6753d89d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f05d60d0049548688f0c2b653bbf7bf5",
            "placeholder": "​",
            "style": "IPY_MODEL_cd951a4f5dfb4205bcdecedbb6c05000",
            "value": "Generating train split: 100%"
          }
        },
        "5e0c241e1d994604852abc08ad6d8244": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f1f2a8dfc9d4d7aa06f2cd506d58304",
            "placeholder": "​",
            "style": "IPY_MODEL_d07eac00c6464627acf8c91820e0d33d",
            "value": " 5.73M/5.73M [00:00&lt;00:00, 63.2MB/s]"
          }
        },
        "6657b866176a4eaea7f5c0c1319c882d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e08926aac1141a684bd95dfae430b08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7404303c4ef647d1a04e19a992545270": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bda257be65d475e88759738a2f02434": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c13dafdc47ca427bbe174e85b7c99cb3",
            "placeholder": "​",
            "style": "IPY_MODEL_0fe546c5595f493abbd9b3085e03c1b9",
            "value": "train-00000-of-00001.parquet: 100%"
          }
        },
        "7f1f2a8dfc9d4d7aa06f2cd506d58304": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8068edb79aeb4924adefebfe737f662a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80cbc6d514a543beb16f14b28a8dbf86": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "958e21e68fdb4c5b9f5cdb3c4ae54e6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a71c9a70715440f7be0ee36a69ebaa98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b332b3772a664901b9870841b96440bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c13dafdc47ca427bbe174e85b7c99cb3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb9267758a074b53b6e4792ddae9ca85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7045851da074c86a4d96f130d414bdc",
            "max": 32332,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6657b866176a4eaea7f5c0c1319c882d",
            "value": 32332
          }
        },
        "cd3d42e3afd646618295805eff5f2027": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd951a4f5dfb4205bcdecedbb6c05000": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d07eac00c6464627acf8c91820e0d33d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4eef6b7f0a24d02a23845b99ac4978c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14002e17454245beb17ca33393a6d4e3",
            "placeholder": "​",
            "style": "IPY_MODEL_135b5690d178472785fdbd679ac009c7",
            "value": "README.md: 100%"
          }
        },
        "e581731685c2434ab864c77b93900987": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57b1e06305bd4f9c9f5d120a45b286f0",
            "max": 5726189,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a71c9a70715440f7be0ee36a69ebaa98",
            "value": 5726189
          }
        },
        "e5fd79a50c3d4fd19d3df19862238109": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80cbc6d514a543beb16f14b28a8dbf86",
            "max": 28064,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_958e21e68fdb4c5b9f5cdb3c4ae54e6e",
            "value": 28064
          }
        },
        "e7045851da074c86a4d96f130d414bdc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f05d60d0049548688f0c2b653bbf7bf5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
